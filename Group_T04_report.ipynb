{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basics of Mobile Robotics Final Project** \n",
    "## **Course: \"Basics of Mobile Robotics\"(MICRO-452)**\n",
    "## **Professor: Francesco Mondada**\n",
    "## **EPFL - December 2023**\n",
    "### **Authors:**\n",
    "- Evangelista Santiago Roberto\n",
    "\n",
    "- Giovine Angelo\n",
    "\n",
    "- He Weifeng\n",
    "\n",
    "- Syla Valmir\n",
    "\n",
    "## **Tables of contents**\n",
    "\n",
    "- [Demo Videos](#Demo-Videos)\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "  - [Project description and details](#Project-description-and-details)\n",
    "  - [Modules and libraries required](#Modules-and-libraries-required)\n",
    "\n",
    "- [Environment](#Environment)\n",
    "\n",
    "- [Vision](#Vision)\n",
    "  - [Camera](#Camera)\n",
    "  - [Environment mapping](#Environment-mapping)\n",
    "  - [Global obstacle detection](#Global-obstacle-detection)\n",
    "  - [Robot detection](#Robot-detection)\n",
    "  - [Goal detection](#Goal-detection)\n",
    "\n",
    "  \n",
    "\n",
    "- [Global Navigation](#Global-Navigation)\n",
    "  - [Visibility graph](#Visibility-graph)\n",
    "\n",
    "\n",
    "\n",
    "- [Local Navigation](#Local-Navigation)\n",
    "  - [Local obstacle detection](#Local-obstacle-detection)\n",
    "  - [Kidnapping](#Kidnapping)\n",
    "\n",
    "- [Filtering](#Filtering:-Extended-Kalman-Filter)\n",
    "  - [Why Kalman?](#Deriving-the-state-space-model-of-the-Thymio-Robot)\n",
    "  - [Two different models](#Two-different-models)\n",
    "  - [Deriving the state space model of the Thymio Robot](#Deriving-the-state-space-model-of-the-Thymio-Robot)\n",
    "  - [Dealing with the input](#Dealing-with-the-input)\n",
    "  - [Experimentation to derive $Cl$ and $Cr$](#Experimentation-to-derive-$Cl$-and-$Cr$)\n",
    "  - [Calcultating the covariance matrix of the motion model $Q_t$](#Calcultating-the-covariance-matrix-of-the-motion-model-$Q_t$)\n",
    "  - [Deriving the observation model of the Thymio Robot](#Deriving-the-observation-model-of-the-Thymio-Robot)\n",
    "  - [Calcultating the covariance matrix of the observation model $R_t$ ](#Calcultating-the-covariance-matrix-of-the-observation-model-$R_t$ )\n",
    "  - [Implementing the Extended Kalman Filter](#Implementing-the-Extended-Kalman-Filter)\n",
    "\n",
    "\n",
    "\n",
    "- [Control law ](#Control-law)\n",
    "\n",
    "- [Main](#Main)\n",
    "\n",
    "- [The Code](#The-Code)\n",
    "\n",
    "- [Conclusion](#Conclusion)\n",
    "\n",
    "- [Sources](#Sources)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Demo videos**\n",
    "\n",
    "The optimal path is displayed in white, while the real-time estimated position of the Thymio, obtained through filtering, is indicated by a black dotted line.\n",
    "\n",
    "Full demo + Normal situation + Kidnapping + Camera obstructed + Local obstacles: https://youtu.be/rz_EOHwGMOw\n",
    "\n",
    "Full demo: https://youtu.be/DTvLO2GhbcM\n",
    "    \n",
    "Normal situation: https://youtu.be/M8M3lAdUMPQ\n",
    "\n",
    "Kidnapping: https://youtu.be/5M4zRgAHsiA\n",
    "\n",
    "Camera obstructed: https://youtu.be/vcZKryRRfx0\n",
    "\n",
    "Local obstacles: https://youtu.be/Q0z2UTtgOtY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "#### **Project description and details:**\n",
    "The project is structured around five distinct modules, each tailored to a specific aspect of the Thymio Robot's navigation and operational capabilities:\n",
    "- Vision Module\n",
    "- Global Navigation Module\n",
    "- Local Navigation Module\n",
    "- Filtering Module\n",
    "- Robot control\n",
    "\n",
    "The project uses mainly the camelCase naming convention for variables and functions (everything except the Extended Kalman filter). For the EKF, the snake case convention was used. This was because of the ditribution of the work among the team members.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modules and libraries required:**\n",
    "\n",
    "The following modules and libraries are required to run the code:\n",
    "- numpy\n",
    "- matplotlib\n",
    "- cv2\n",
    "- math\n",
    "- time\n",
    "- pyvisgraph\n",
    "- tdmclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment**\n",
    "\n",
    "For the environment, a monochromatic blue background was selected, complemented by 2D green obstacles and a yellow target area. The Thymio robot is identified by a distinctive red mask applied to it. The environment was constructed on a pool table.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/environment.jpeg\" alt=\"Environment on the pool table\" width=\"400\" style=\"margin-right: 10px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "ChatGPT-4 was used to create polygons as map obstacles. While the model excelled in generating shapes, it faced challenges with non-overlapping shapes in PDFs and random polygon generation. The details were then manually specified, making it a collaborative effort. This experience underscored the limitations of Large Language Models in creative tasks.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/GPT_error.png\" alt=\"GPT error message\" width=\"450\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/GPT_env.png\" alt=\"polygons generated by GPT4 \" width=\"550\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Camera**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/iPhone.jpg\" alt=\"very bad image of the concept\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "The camera utilized in the project differed from the Aukey Stream Series camera provided by the course. Initially,the Aukey camera was used, but during experimentation, it was discovered that when using a MacBook, an iPhone's camera connected to the same iCloud account can be automatically used as a wireless webcam. This feature was particularly advantageous as the camera needed to be mounted on the ceiling to face downwards towards the table. By using the iPhone, it was possible to attach it to the ceiling without to any cables connected to the computer. Additionally, it was observed that the iPhone's camera delivered smoother images compared to the provided camera. In Python, the iPhone camera was recognized as a standard webcam, which meant that no modifications were required in the existing code.\n",
    "#### **Environment mapping**\n",
    "\n",
    "The Computer Vision module in the system is designed to translate the physical environment into a two-dimensional representation. To facilitate this mapping, four small green markers are positioned at the corners of the table. The camera's role is to identify these markers. It's important to note that these markers are uniquely recognized among other green objects in the environment (which are considered global obstacles) by focusing on the four external green objects detected. Once these markers are detected, the system maps them to specific coordinates on the 2D plane. The positioning of these points is adjusted based on the aspect ratio of the markers to ensure accurate representation of the environment.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/beforeMapping.jpg\" alt=\"Environment on the pool table from the camera before\" height=\"300\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/afterMap.jpeg\" alt=\"Environment on the pool table from the camera after\" height=\"300\"/>\n",
    "</div>\n",
    "\n",
    "### **Global obstacle detection**\n",
    "\n",
    "The detection of global obstacles is achieved by identifying green objects within the environment. To accommodate the navigation algorithm employed, which is the visibility graph, these detected objects are enlarged in the system's representation. This enlargement is necessary because the algorithm does not intrinsically account for the physical dimensions of the robot.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/PHOTO_OBJECT.png\" alt=\"green obstacle on the table\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "### **Robot detection**\n",
    "The robot's location is determined by identifying the red mask affixed to it. This mask consists of a white background with two red rectangles: a larger one positioned at the rear, centrally between the two wheels (which is crucial as it marks the robot's center of rotation), and a smaller one at the front. The use of two distinct rectangles allows for distinguishing the robot's orientation within a full 360-degree range ($0$ to $2 \\pi$ radians) rather than just 180 degrees ($0$ to $\\pi$ radians). This technique effectively provides both the position ($x,y$) and the orientation ($\\gamma$) of the Thymio.\n",
    "\n",
    "The Thymio detection can be seen in the figure from the previous section.\n",
    "\n",
    "\n",
    "### **Goal detection**\n",
    "The goal is detected using a yellow marker. The centroid of the largest yellow object is taken as the goal position.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/PHOTO_GOAL.png\" alt=\"yellow obstacle on the table\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Global Navigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visibility graph**\n",
    "For the lobal navigation, the visibility graph algorithm was used. To do that, the library pyvisgraph was used. The library has all the functions needed, including the finding of the shortest path, so no extra libraries (apart from the already used numpy and matplotlib) were needed. It is important to note that the library uses a different coordinate standard than openCV, so an extra function was needed to convert the coordinates. A significant improvement made was extending the points on the edge of the map further away. In initial tests, there were problems when a global obstacle was near an edge, despite expanding the obstacles to account for the robot's size. To address this, the points on the global obstacles that are close to the edge were moved a considerable distance outward, away from the edge. This adjustment guaranteed that the path planning algorithm would not take these points into account.\n",
    "<!--  -->\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/seenFromCam.png\" alt=\"Image seen from the camera\" height=\"300\"/>\n",
    "    <img src=\"img/pointsInPath.png\" alt=\"Plot with points in the shortest path\" height=\"300\" style=\"margin-right: 10px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Local Navigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Local obstacle detection**\n",
    "\n",
    "To detect the local obstacles the horizontal distance sensors in the front of the robot were used. Depending on in which side the obstacle is detected, the robot will turn in the opposite direction and avoid the obstacle using a squared path manouver. The moving of the robot was done using a direct speed input and timers, since it proved to be simple but effective. The values of the timers were found empirically. Even if considerably precise, at the end of the manouver and once the obstacle has been avoided, the global path planning algorithm is run again to ensure that the robot recovers in the best possible manner. This method has some limitations though. The approximated size of the obstacles has to be known a priori and sufficient place to do the manouver is needed.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Local.png\" alt=\"local\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Kidnapping**\n",
    "\n",
    "\n",
    "To detect the kidnapping of the robot the ground proximity sensors were used. If the robot is lifted from the table a kidnapped flag is activated and while the robot is not positioned back on the table it turns off the motors and waits. When the robot is placed back on the table, a few seconds are waited until the robot is stable and no other perturbations are present (e.g. hands that can be confused as the robot, causing problem with the path planning).In the end, the global path planning algorithm is run again to find the new path to the goal. \n",
    "\n",
    "The vertical distance sensors were used in the ground reflected mode. After experimentation it was remarked that this was the mode that showed the most difference when the robot was placed on the table or lifted from it, facilitating the determination of the threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Filtering: Extended Kalman Filter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Kalman?**\n",
    "\n",
    "During the course different Bayes filters were presented to estimate the state of a robot in a given environment. For example, a particle filter could have been used, but it is more computationally expensive and it is not necessary for this application. Considering the project requirements, the Kalman filter was finally chosen. This is a recursive filter that estimates the state of a system from a series of noisy measurements. It is a very efficient algorithm that can be used to solve the localization problem of a robot, but makes a fundamental assumption about the distribution of the noise of the various sensors. In fact, it assumes that the noise is Gaussian distributed, which is not always the case in practice.\n",
    "\n",
    "In the presented case, odometry errors can arise from various sources:\n",
    "\n",
    "- Sensor measurement errors: The sensors might not be perfectly accurate or might exhibit some drift over time.\n",
    "- Mechanical Misalignments: Slight imperfections in the robot's construction or wear in components can cause discrepancies.\n",
    "- Slippage Error: The wheels may slip or lose traction, leading to inaccurate distance measurements.\n",
    "\n",
    "Each of these error sources contributes in a small and independent manner to the general error. According to the central limit theorem, when a large number of small, independent effects are summed, their overall distribution tends to approach a Gaussian distribution. This implies that even if each individual source of error might not be Gaussian, their cumulative effect tends towards a Normal distribution. \n",
    "\n",
    "Considering the above, the Kalman filter seems to be a good choice for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Two different models**\n",
    "In the initial stages of the project, an Extended Kalman filter was developed based on a 3-state space model of the Thymio robot. This model incorporated state variables $x$ and $y$ for position, and $\\gamma$ for orientation. As the project advanced, there was a shift towards a more sophisticated 5-state model, which added two additional variables: $v$ for forward velocity, and $\\omega$ for angular velocity, to better integrate odometry data.\n",
    "\n",
    "Despite the increased complexity and completeness of this new 5-state approach, the difference in functionality compared to the original 3-state model was not markedly significant. The 5-state model provided improvements, especially in terms of data integration, but the core functionality and effectiveness of the Extended Kalman filter remained relatively similar in both models.\n",
    "\n",
    "To conclude, our project encompasses two versions of Kalman filters: one designed for the simpler 3-state model and another for the advanced 5-state model. This report will concentrate on the 5-state model, which represents an evolution from the original 3-state concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deriving the state space model of the Thymio Robot**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/state_space_.png\" alt=\"2D image of a differential drive robot taken from Automatic Addison\" width=\"400\"/>\n",
    "    <br>\n",
    "    <div> 2D differential drive robot, source: Automatic Addison</div>\n",
    "</div>\n",
    "\n",
    "\n",
    "As introduced in the previous paragraph the following state variables will be used: $x$, $y$ for the position, $\\gamma$ for the orientation, $v$ forward velocity, and $\\omega$ angular velocity. Since the actuators of the Thyimio are controlled in speed, the input variables $v$  and $\\omega$ can be defined. Morover $\\Delta t$ is the sampling time and $t$ refers to the current time instant. After some analysis the state vector $s_t$ is defined as follows: \n",
    "\n",
    "$$\n",
    "s_{t} =\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t} \\\\\n",
    "v_{t} \\\\\n",
    "\\omega_{t}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{t-1} + v_{t-1}\\cos (\\gamma_{t-1})  \\Delta t \\\\\n",
    "y_{t-1} + v_{t-1}\\sin (\\gamma_{t-1})  \\Delta t \\\\\n",
    "\\gamma_{t-1} + \\omega_{t-1}  \\Delta t \\\\\n",
    "v_{t-1} \\\\\n",
    "\\omega_{t-1}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "f_1 \\\\\n",
    "f_2 \\\\\n",
    "f_3 \\\\\n",
    "f_4 \\\\\n",
    "f_5\n",
    "\\end{bmatrix}\n",
    "=\n",
    "f( s_{t-1},  u_{t-1})\n",
    "$$\n",
    "\n",
    "That can be written as:\n",
    "$$s_{t}= A_{t-1} \\cdot s_{t-1} + B_{t-1} \\cdot u_{t-1} $$\n",
    "\n",
    "$$\n",
    "s_{t} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0& 0  & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{t-1} \\\\\n",
    "y_{t-1} \\\\\n",
    "\\gamma_{t-1} \\\\\n",
    "v_{t-1} \\\\\n",
    "\\omega_{t-1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\cos (\\gamma_{t-1}) \\Delta t & 0 \\\\ \n",
    "\\sin (\\gamma_{t-1})  \\Delta t & 0 \\\\\n",
    "0 & \\Delta t \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    v_{t-1} \\\\ \n",
    "     \\omega_{t-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence: \n",
    "$$\n",
    "A_{t-1} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0& 0  & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    ", \\quad\n",
    "B_{t-1} = \n",
    "\\begin{bmatrix}\n",
    "\\cos(\\gamma_{t-1} ) \\Delta t & 0 \\\\\n",
    "\\sin(\\gamma_{t-1} ) \\Delta t & 0 \\\\\n",
    "0 & \\Delta t \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Since the system is non-linear, the Jacobian of the state transition function $f$ needs to be calculated in order to implement the Extended Kalman filter. This is done by taking the partial derivatives of $f$ with respect to each state variable. The Jacobian matrix is then defined as:\n",
    "$$\n",
    "\\frac{\\partial f( s_{t-1},  u_{t-1})}{\\partial s_{t-1}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_{t-1}} & \\frac{\\partial f_2}{\\partial y_{t-1}}  & \\frac{\\partial f_3}{\\partial \\gamma_{t-1}} & \\frac{\\partial f_4}{\\partial v_{t-1}} & \\frac{\\partial f_5}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_{t-1}} & \\frac{\\partial f_2}{\\partial y_{t-1}}  & \\frac{\\partial f_3}{\\partial \\gamma_{t-1}} & \\frac{\\partial f_4}{\\partial v_{t-1}} & \\frac{\\partial f_5}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_{t-1}} & \\frac{\\partial f_2}{\\partial y_{t-1}}  & \\frac{\\partial f_3}{\\partial \\gamma_{t-1}} & \\frac{\\partial f_4}{\\partial v_{t-1}} & \\frac{\\partial f_5}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_{t-1}} & \\frac{\\partial f_2}{\\partial y_{t-1}}  & \\frac{\\partial f_3}{\\partial \\gamma_{t-1}} & \\frac{\\partial f_4}{\\partial v_{t-1}} & \\frac{\\partial f_5}{\\partial \\omega_{t-1}} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_{t-1}} & \\frac{\\partial f_2}{\\partial y_{t-1}}  & \\frac{\\partial f_3}{\\partial \\gamma_{t-1}} & \\frac{\\partial f_4}{\\partial v_{t-1}} & \\frac{\\partial f_5}{\\partial \\omega_{t-1}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & - \\Delta t \\sin(\\gamma_{t-1}) v_{t-1} & \\cos(\\gamma_{t-1} ) \\Delta t& 0\\\\\n",
    "0 & 1 & \\Delta t \\cos(\\gamma_{t-1}) v_{t-1} & \\sin(\\gamma_{t-1} ) \\Delta t & 0\\\\\n",
    "0 & 0 & 1 & 0 & \\Delta t\\\\\n",
    "0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "If the process noise  $ w_{t} = [ w^1_{t}, w^2_{t}, w^3_{t}, w^4_{t}, w^5_{t} ] $\n",
    " is also considered, the state space model can be then defined as:\n",
    "$$s_{t}= A_{t-1} \\cdot s_{t-1} + B_{t-1} \\cdot u_{t-1} +w_{t-1}$$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{t} \\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t} \\\\\n",
    "v_{t} \\\\\n",
    "\\omega_{t}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0& 0  & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{t-1} \\\\\n",
    "y_{t-1} \\\\\n",
    "\\gamma_{t-1} \\\\\n",
    "v_{t-1} \\\\\n",
    "\\omega_{t-1} \n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\gamma_{t-1} ) \\Delta t & 0 \\\\\n",
    "\\sin(\\gamma_{t-1} ) \\Delta t & 0 \\\\\n",
    "0 & \\Delta t \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{t-1} \\\\\n",
    "\\omega_{t-1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "w^1_{t-1} \\\\\n",
    "w^2_{t-1} \\\\\n",
    "w^3_{t-1} \\\\\n",
    "w^4_{t-1} \\\\\n",
    "w^5_{t-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that it is assumed that the process noise is zero-mean Gaussian noise with covariance matrix $Q_t$, which was computed empirically with a process explained in the next sections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dealing with the inputs**\n",
    "\n",
    "It is important to observe that the state space model is controlled in $v$ forward velocity, and $\\omega$ angular velocity. In reality only the speed of the two motors $v_{r}$, and $v_{l}$ of the Thymio can be controlled. A conversion between the speed of the robot $v$ and $\\omega$ into $v_{r}$, and $v_{l}$ is then derived using the following equations:\n",
    "\n",
    "$v_{r} = \\frac{2v+\\omega L}{2R}$ , $v_{l} = \\frac{2v-\\omega L}{2R}$\n",
    "\n",
    "Where $L$ is the distance between the two wheels and $R$ is the radius of the wheels.\n",
    "\n",
    "In order to control the robot correctly, the multiplication factors $C_r$ and $C_l$ are added to the equations in the code to account for the unit change between the speed and the inputs of the robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experimentation to derive $Cl$ and $Cr$**\n",
    "\n",
    "In order to use the $Cl$ and $Cr$ values in the program, they need to be obtained empirically. This section contains the collected data and the results. First, each wheel of the Thymio was marked. Then, the robot speed was fixed at a constant value of $200$  $Thymio units$ and the number of revolutions of each wheel in a fixed time interval $t$ were measured. The experiment was done 10 times and the average values of $Cl$ and $Cr$ were calculated.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Thymio_wheel.jpeg\" alt=\"Description of Thymio wheel\" width=\"500\" style=\"margin-right: 10px;\"/>\n",
    "    <img src=\"img/Cl_Cr.png\" alt=\"Description of second image\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "$$Cl = 69.33821285 \\frac{s}{rad}Thymio units,Cr = 70.74580573 \\frac{s}{rad}Thymio units$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calcultating the covariance matrix of the motion model $Q_t$**\n",
    "\n",
    "$$\n",
    "Q_t =\n",
    "\\begin{bmatrix}\n",
    "Q_t^1 & 0 \\\\\n",
    "0 & Q_t^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It would be helpful wrote the covariance matrix $Q_t$ as a block matrix, where $Q_t^1$ is the covariance matrix of the variables $x$, $y$ and $\\gamma$ and $Q_t^2$ is the covariance matrix of the variables $v$ and $\\omega$. \n",
    "$$\n",
    "Q_t^1 = \n",
    "\\begin{bmatrix}\n",
    "Var(x_t) & Cov(x_t, y_t) & Cov(x_t, \\gamma_t) \\\\\n",
    "Cov(y_t, x_t) & Var(y_t) & Cov(y_t, \\gamma_t) \\\\\n",
    "Cov(\\gamma_t, x_t) & Cov(\\gamma_t, y_t) & Var(\\gamma_t)\n",
    "\\end{bmatrix}\n",
    ", \\quad\n",
    "Q_t^2 = \n",
    "\\begin{bmatrix}\n",
    "Var(v_t) & Cov(v_t,w_t) \\\\\n",
    "Cov(w_t,v_t) & Var(w_t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The covariance matrix $Q_t^ 2$ is calculated empirically, since it's possible to have access to the speed of the $2$ motors, the parameters $Var(v_r)$ and $Var(v_l)$ can be obtained empirically. By collecting data from the Thymio proceeding at a constant speed of $200$ $Thymiounits$, the variance of the two motors were obtained in $(rad/s)^2$: \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/motor_speed.png\" alt=\"Description of the image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "$$Var(v_r) = 0.1399(rad/s)^2$$\n",
    "$$Var(v_l) = 0.0713(rad/s)^2$$\n",
    "\n",
    "Now by simply knowing the geometric relation between $v_r$, $v_l$ and $v$, $\\omega$, the covariance matrix $Q_t^2$ can be obtained:\n",
    "\n",
    "$$Var(v) = 0.3007 (mm/s)^2$$\n",
    "$$Var(\\omega) = 0.0180 (rad/s)^2 $$\n",
    "\n",
    "$$\n",
    "Q_t^2 = \n",
    "\\begin{bmatrix}\n",
    "0.3007 & 0 \\\\\n",
    "0 & 0.0180\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The term $Cov(v,\\omega)$ was also calculated through error propagation approach, but its contribution was near to zero, so it was neglected.\n",
    "\n",
    "It's important to observe that the covariance matrix $Q_t^2$ would coincide with the second block of my $R_t$ matrix, since speed measurement of the Thymio is obtained through the odometry system. \n",
    "\n",
    "The covariance matrix $Q_t^1$ is calculated using the propagation of the error:\n",
    "\n",
    "while the odometry system provides only the velocity of the two motors, the geometry of the robot can be used to calculate its position and orientation, hence: $$x=f_1(v_r,v_l) = \\frac{R}{2} (v_r + v_l) \\cos(\\gamma) \\Delta t$$ $$y=f_2(v_r,v_l) = \\frac{R}{2} (v_r + v_l) \\sin(\\gamma) \\Delta t$$ $$\\gamma=f_3(v_r,v_l) =\\frac{R}{L} (v_r-v_l) \\Delta t$$ where $v_r$ and $v_l$ are the velocity of the two motors, $R$ is the radius of the wheel and $L$ is the distance between the two wheels . The propagation error is then derived: $$Var(x) = (\\frac{\\partial f_1}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l})^2 Var(v_l) +2(\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) $$ $$Var(y) = (\\frac{\\partial f_2}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_2}{\\partial v_l})^2 Var(v_l) +2(\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) $$ $$Var(\\gamma) = (\\frac{\\partial f_3}{\\partial v_r})^2 Var(v_r) + (\\frac{\\partial f_3}{\\partial v_l})^2 Var(v_l) +2 (\\frac{\\partial f_3}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l)  $$ \n",
    "\n",
    "$v_r$ and $v_l$ are considered uncorrelated, so after deriving the partial derivatives the diagonal terms of $Q_t^1$ become:\n",
    "\n",
    "$$Var(x) =  (\\frac{R}{2} \\cos(\\gamma) \\Delta t)^2 (Var(v_r)+Var(v_l))$$ \n",
    "$$Var(y) = (\\frac{R}{2} \\sin(\\gamma) \\Delta t)^2 (Var(v_r)+Var(v_l))$$ \n",
    "$$Var(\\gamma) =  (\\frac{R}{L}\\Delta t)^2 (Var(v_r)+Var(v_l))$$\n",
    "\n",
    "The off-diagonal terms of $Q_t^1$ can then be calculated (taking in account the error propagation):\n",
    "$$ Cov(x,y) = Cov(y,x)  = (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_2}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_2}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_2}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_2}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "$$ Cov(x,\\gamma) = Cov(\\gamma,x) = (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_1}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_1}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "$$ Cov(y,\\gamma) = Cov(\\gamma,y) = (\\frac{\\partial f_2}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_r}) Var(v_r) + (\\frac{\\partial f_2}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_l}) Var(v_l) + (\\frac{\\partial f_2}{\\partial v_r}) (\\frac{\\partial f_3}{\\partial v_l}) Cov(v_r,v_l) + (\\frac{\\partial f_2}{\\partial v_l}) (\\frac{\\partial f_3}{\\partial v_r}) Cov(v_l,v_r)$$\n",
    "\n",
    "After deriving the partial derivatives the following equations are obtained:\n",
    "\n",
    "$$ Cov(x,y) = \\frac{(R  \\Delta t)}{4}^2(\\cos(\\gamma)\\sin(\\gamma))(Var(v_r)+Var(v_l)) $$\n",
    "$$ Cov(x,\\gamma) = \\frac{(R \\Delta t)^2}{2L} \\cos(\\gamma) (Var(v_r)-Var(v_l) )$$\n",
    "$$ Cov(y,\\gamma) = \\frac{(R \\Delta t)^2}{2L} \\sin(\\gamma) (Var(v_r)-Var(v_l)) $$\n",
    "\n",
    "\n",
    "After some simulation for different values of $\\gamma$ from $0°$ to $180°$ it was found that the matrix $Q_t^1$ is diagonal. (The off-diagonal terms are very small compared to the diagonal terms, that's very clear from the graph below). \n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/Q.png\" alt=\"Description of the image\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Through simulation,  a matrix $Q_t^1 = Q^1$ was selected with values that maximize the variance of each variable, thereby obtaining a conservative estimate of the matrix.\n",
    "\n",
    "$$\n",
    "Q^1 =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 0.7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the end, the covariance matrix $Q$ is defined as:\n",
    "$$\n",
    "Q =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 2 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0.7 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0.3007 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0.0180\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deriving the observation model of the Thymio Robot**\n",
    "\n",
    "Now that a state space model that includes also the process noise has been derived, an observation model is required in order to be able to implement the Extended Kalman filter. \n",
    "\n",
    "An observation model describes how the sensor outputs $y_t$ are related to the state vector $s_t$, considering also a vector of observation noise $ \\nu_{t} = [ \\nu^1_{t}, \\nu^2_{t}, \\nu^3_{t}, \\nu^4_{t} , \\nu^5_{t}  ] ^T$\n",
    "with zero mean and covariance matrix $R$ (which is computed through experimentation, explained in the next section). \n",
    "The observation model is defined as follows: \n",
    "$$y_t = H_t s_t + \\nu_t$$ \n",
    "Since it is possible to detect the position and orientation of the Thyimio in the environment using the camera, the matrix first $3x3$ block of the $H$ matrix becomes the identity matrix, and if we consider that we can recive the velocity of the Thymio from the velocity sensor, also the second $2x2$ block of the $H$ matrix is the identity matrix hence the observation model can be defined as follows:\n",
    "\n",
    "$$\n",
    "y_{t} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x_{t}\\\\\n",
    "y_{t} \\\\\n",
    "\\gamma_{t} \\\\\n",
    "v_{t} \\\\\n",
    "\\omega_{t}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\nu^1_{t}\\\\\n",
    "\\nu^2_{t}\\\\\n",
    "\\nu^3_{t} \\\\\n",
    "\\nu^4_{t} \\\\\n",
    "\\nu^5_{t}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It's important to notice that the system is robust even to the absence of camera measurements, even if the matrix $H$ is not changed. More details on this topic are provided in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calcultating the covariance matrix of the observation model $R_t$**\n",
    "\n",
    "The observation vector is denoted as follows:\n",
    "\n",
    "$$\n",
    "z_{t} =\n",
    "\\begin{bmatrix}\n",
    "x^o_{t} \\\\\n",
    "y^o_{t} \\\\\n",
    "\\gamma^o_{t} \\\\\n",
    "v_{t} \\\\\n",
    "\\omega_{t} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The apice $o$ is used to indicate that the values are obtained from the camera, while no apice is used to indicate that the values are obtained from the velocity sensor.\n",
    "\n",
    "\n",
    "Due to the previous consideration the variance of the observation will then be:\n",
    "\n",
    "$$\n",
    "R_t^1 =\n",
    "\\begin{bmatrix}\n",
    "R_t^1 & 0 \\\\\n",
    "0 & R_t^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $R_t^2 = Q_t^2$ is the covariance matrix of the velocity sensor, while $R_t^1$ is the covariance matrix of the camera. \n",
    "\n",
    "$$\n",
    "R_t^1 = \n",
    "\\begin{bmatrix}\n",
    "Var(x^o) & Cov(x^o, y^o) & Cov(x^o, \\gamma^o) \\\\\n",
    "Cov(y^o, x^o) & Var(y^o) & Cov(y^o, \\gamma^o) \\\\\n",
    "Cov(\\gamma^o, x^o) & Cov(\\gamma^o, y^o) & Var(\\gamma^o)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Note that those terms are different from the ones of the motion model because the observation noise is related to the camera and the computer vision setup and not to the Thymio. This matrix changes in relation to the visibility of the robot. Two different scenarios are considered: when the Thymio is visible and when it is not. Consequently $2$ matrices $R_t^1$ and $R_{nc}$ are defined. If the Thymio is not visible the incertaitnty grows to infinity, corresponding to the case using matrix $R_{nc}$ :\n",
    "$$\n",
    "R_{nc} = \n",
    "\\begin{bmatrix}\n",
    "\\infty &0 & 0 & 0 & 0\\\\\n",
    "0 & \\infty & 0 & 0 & 0\\\\\n",
    "0 & 0 & \\infty & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0.3007 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0.0180\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now it is possible to compute $R_t^1 = R^1$ empirically. To do this, first a callibration grid in an A4 sheet was created, ensuring an accurate positioning of the Thymio robot before detecting its location and orientation. The A4 paper was generated using ChatGPT-4. The LLM generated the following file after the requirements were specified:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/square_design.png\" alt=\"Description of the image\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "After this, the required variance was computed mesuring the delta between the measure of the Thymio provided by the camera and its real position. The mesurment was repeated 10 times and the average values for $Var(x^o)$, $Var(y^o)$ and $Var(\\gamma^o)$ were calculated. To be consistent with the units of the motion model, the camera measurements have been converted from pixels to millimeters.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/R_exp.jpg\" alt=\"Description of the image\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$Var(x^o) =0.7 (mm)^2$$\n",
    "$$Var(y^o) = 0.7 (mm)^2$$\n",
    "$$Var(\\gamma^o) = 0.014 (rad)^2$$\n",
    "\n",
    "Hence the $R^1$ matrix is:\n",
    "$$\n",
    "R^1 = \n",
    "\\begin{bmatrix}\n",
    "0.7 &0 & 0 \\\\\n",
    "0 & 0.7 & 0 \\\\\n",
    "0 & 0 & 0.014\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the end we obtain the covariance matrix $R$:\n",
    "$$\n",
    "R =\n",
    "\\begin{bmatrix}\n",
    "0.7 &0 & 0 & 0 & 0\\\\\n",
    "0 & 0.7 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0.014 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0.3007 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0.0180\n",
    "\\end{bmatrix}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementing the Extended Kalman Filter**\n",
    "\n",
    "Since the model is not linear, the implementation of an Extended Kalman Filter (EKF) is required.\n",
    "\n",
    "The EKF as, the standard Kalman filter, is composed of two steps: the prediction step and the update step. \n",
    "\n",
    "\n",
    "Prediction: The EKF estimates the system's current state and uncertainty by applying a nonlinear state transition model to the previous state. The process linearizes the nonlinear function using the Jacobian of the state transition function, for this reason in the code the function getJacobianA was implemented.\n",
    "\n",
    "Update: Upon receiving new measurements, the EKF updates the state estimate and reduces uncertainty. Since the observation model is linear, this step is the same of a standard KF, hence during this step, the algorithm calculates the Kalman Gain, which adjusts the prediction based on the new data.\n",
    "\n",
    "It's important to notice that since my model is non-linear (taking the jacobian of the state function correspond to use an approximation of it), the EKF loses the optimality property of the standard Kalman filter, but it is a valid and widely used estimator in Mobile Robotics applications.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Robot control**\n",
    "\n",
    "Initially, a simple P controller was employed for the control law, aiming to minimize the perpendicular distance from the robot to the ideal trajectory line. However, this controller led to instability issues. Consequently, it was decided to switch to a proportional-integral (PI) controller for the angle and a proportional (P) controller for the distance to the optimal path, combined. A differential speed is applied to the wheels of the robot based on the output of the controller.\n",
    "\n",
    "This controller functions based on the angle error between the desired angle and the current angle of the Thymio robot, along with the perpendicular distance from the Thymio to the line it should be following. Two main modes govern the controller's operation: alignment mode and forward mode.\n",
    "\n",
    "Alignment Mode: In this mode, the controller exclusively addresses the angle error. It is utilized when the robot reaches each point in the path, preparing for the next segment.\n",
    "\n",
    "\n",
    "Forward Mode: In this mode, the controller considers both the angle error and the perpendicular distance to the line. It becomes active when the robot is in motion between two points in the path.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/control.png\" alt=\"Control error diagram\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main**\n",
    "\n",
    "The main function is the core of the project. It is responsible for the initialization of the different modules and the communication between them.\n",
    "\n",
    "The function follows the concept of operations presented in the figure below:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"img/ConOps.png\" alt=\"Cocept of operations\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from src.computerVision import correctPerspectiveStream, findCorners, getPerspectiveMatrix, findThymio, findGlobalObstacles, findGoal\n",
    "from src.kalman import estimatePosition, inverseSpeedConversion, r11, r22, r33, Cl, Cr, R, L,r44,r55\n",
    "from src.pathPlanning import buildGraph\n",
    "from src.robotControl import robotController, checkForObstacles\n",
    "\n",
    "XYMIRROR = False\n",
    "IMAGE_WIDTH = 1920\n",
    "IMAGE_HEIGHT = 1080\n",
    "POSITION_THRESHOLD = 50\n",
    "KIDNAPPING_THRESHOLD = 100\n",
    "KIDNAPPING_TIME = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()\n",
    "\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def motors(l_speed=500, r_speed=500, verbose=False):\n",
    "\n",
    "    global motor_left_target, motor_right_target\n",
    "\n",
    "    l_speed = int(l_speed)\n",
    "    r_speed = int(r_speed)\n",
    "    \n",
    "    motor_left_target = l_speed\n",
    "    motor_right_target = r_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "@tdmclient.notebook.sync_var \n",
    "def motorSpeeds():\n",
    "    global motor_left_speed, motor_right_speed    \n",
    "    return motor_left_speed, motor_right_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "@tdmclient.notebook.sync_var \n",
    "def horiz_sensor():\n",
    "    global prox_horizontal\n",
    "    return prox_horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "@tdmclient.notebook.sync_var \n",
    "def getVerticalDistance():\n",
    "    global prox_ground_reflected\n",
    "    return prox_ground_reflected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_python\n",
    "\n",
    "# Note: This function was provided by the course Basics Of Mobile Robotics by Prof. Francesco Mondada\n",
    "# Source: https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "nf_leds_prox_h(0,0,0,0,0,0,0,0) \n",
    "nf_leds_rc(0)\n",
    "nf_leds_temperature(0, 0)\n",
    "nf_leds_bottom_right(0,0,0)\n",
    "nf_leds_bottom_left(0,0,0)\n",
    "nf_leds_top(0,0,0)\n",
    "nf_leds_prox_v(0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: these functions are defined here and not in their respective files because they call the functions that interact with the robot defined above\n",
    "\n",
    "def checkForKidnap():\n",
    "    ver=getVerticalDistance()\n",
    "    if ver[0] < KIDNAPPING_THRESHOLD or ver[1] < KIDNAPPING_THRESHOLD:\n",
    "        print(\"Kidnapped!\")\n",
    "        motors(0,0)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def avoidObstacle(leftOrRight, initTime, currentTime):\n",
    "    motors(0,0)\n",
    "    \n",
    "    tTurn = 1\n",
    "    tStraight = 2\n",
    "    vStraight = 200\n",
    "\n",
    "    if leftOrRight == 'right':\n",
    "        l=-200\n",
    "        r=200\n",
    "    elif leftOrRight == 'left':\n",
    "        l=200\n",
    "        r=-200\n",
    "\n",
    "    motors(l_speed=l, r_speed=r)\n",
    "    if currentTime - initTime < tTurn:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=vStraight, r_speed=vStraight)\n",
    "    if currentTime - initTime < tTurn + tStraight:\n",
    "        return True\n",
    "    \n",
    "\n",
    "    motors(l_speed=-l, r_speed=-r)\n",
    "    if currentTime - initTime < 2*tTurn + tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=vStraight, r_speed=vStraight)\n",
    "    if currentTime - initTime < 2*tTurn + 2.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=-l, r_speed=-r)\n",
    "    if currentTime - initTime < 3*tTurn + 2.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=vStraight, r_speed=vStraight)\n",
    "    if currentTime - initTime < 3*tTurn + 3.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=l, r_speed=r)\n",
    "    if currentTime - initTime < 4*tTurn + 3.5*tStraight:\n",
    "        return True\n",
    "\n",
    "    motors(l_speed=0, r_speed=0)\n",
    "\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "previousTime = time.time()\n",
    "\n",
    "#state control variables\n",
    "correctedCam = False\n",
    "environmentSetup = False\n",
    "redoPath = False\n",
    "avoidingObstacle = False\n",
    "wasKidnapped = False\n",
    "aligned = False\n",
    "\n",
    "#initializing variables\n",
    "path = []\n",
    "goal = np.array([0,0])\n",
    "lSpeed = 0\n",
    "rSpeed = 0\n",
    "P_k = np.array([[r11,0,0,0,0],[0,r22,0,0,0],[0,0,r33,0,0],[0,0,0,r44,0],[0,0,0,0,r55]])\n",
    "\n",
    "postionHistory = []\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    key = cv2.waitKey(1)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Error: failed to capture image\")\n",
    "        break\n",
    "\n",
    "    if not correctedCam:\n",
    "        # Map the image to the corner\n",
    "        position, angle, _=findThymio(frame)\n",
    "        estimateState = np.array([position[0],IMAGE_HEIGHT-position[1],angle,0,0])\n",
    "        cv2.imshow('Thymio Camera', frame)\n",
    "        try:\n",
    "            centroids = findCorners(frame)\n",
    "            perspectiveMatrix = getPerspectiveMatrix(centroids)\n",
    "            correctedCam = True\n",
    "        except:\n",
    "            print(\"Fail\")\n",
    "            continue\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Correct the perspective of the image\n",
    "    if correctedCam:\n",
    "        frame = correctPerspectiveStream(frame,perspectiveMatrix)\n",
    "        frameToPlot = frame\n",
    "\n",
    "\n",
    "\n",
    "    current_time = time.time()\n",
    "    dt = current_time - previousTime\n",
    "    if dt < 0.15:\n",
    "        time.sleep(0.15-dt)\n",
    "\n",
    "    previousTime = current_time\n",
    "\n",
    "\n",
    "    previousControlVector = inverseSpeedConversion(rSpeed,lSpeed,R,L,Cr,Cl)\n",
    "\n",
    "    measLS, measRS = motorSpeeds()\n",
    "\n",
    "    position, angle, estimateState, P_k = estimatePosition(frame,previousControlVector,dt,P_k,estimateState, measLS, measRS)\n",
    "\n",
    "    postionHistory.append(position)\n",
    "\n",
    "\n",
    "\n",
    "    #plot the points in position history in black\n",
    "    for i in range(len(postionHistory)-1):\n",
    "        cv2.circle(frameToPlot, (int(postionHistory[i][0]), int(IMAGE_HEIGHT-postionHistory[i][1])), 5, (0, 0, 0), -1)\n",
    "\n",
    "    #plot the goal\n",
    "    cv2.circle(frameToPlot, (int(goal[0]), int(goal[1])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    #plot the path lines\n",
    "    for i in range(len(path)-1):\n",
    "        cv2.line(frameToPlot,(int(path[i][0]),int(IMAGE_HEIGHT-path[i][1])),(int(path[i+1][0]),int(IMAGE_HEIGHT-path[i+1][1])),(255,255,255),2)\n",
    "        cv2.circle(frameToPlot, (int(path[i][0]), int(IMAGE_HEIGHT-path[i][1])), 5, (255, 255, 255), -1)\n",
    "        \n",
    "    cv2.imshow('Thymio Camera', frameToPlot)\n",
    "    \n",
    "    \n",
    "    if checkForKidnap():\n",
    "        #cambiare le condizioni\n",
    "        print(\"kidnapped\")\n",
    "        lSpeed = 0\n",
    "        rSpeed = 0\n",
    "        wasKidnapped = True\n",
    "        continue\n",
    "\n",
    "\n",
    "    if wasKidnapped:\n",
    "        time.sleep(KIDNAPPING_TIME)\n",
    "        wasKidnapped = False\n",
    "        redoPath = True\n",
    "        continue\n",
    "\n",
    "\n",
    "    if not avoidingObstacle:\n",
    "        leftOrRight = checkForObstacles(horiz_sensor())\n",
    "\n",
    "\n",
    "    if leftOrRight != None or avoidingObstacle:\n",
    "        if not avoidingObstacle:\n",
    "            initTime = time.time()\n",
    "        avoidingObstacle = avoidObstacle(leftOrRight, initTime, time.time())\n",
    "        redoPath = True\n",
    "        continue\n",
    "    \n",
    "\n",
    "    if (key == ord(' ') and environmentSetup == False) or redoPath:\n",
    "        # Capture an image\n",
    "        imagePath = 'capturedImage.jpg'\n",
    "        cv2.imwrite(imagePath, frame)\n",
    "\n",
    "        capturedImage = cv2.imread(imagePath)\n",
    "        \n",
    "        position, angle, estimateState, P_k = estimatePosition(capturedImage,previousControlVector,dt,P_k,estimateState, measLS, measRS)\n",
    "\n",
    "        greenObjects = findGlobalObstacles(capturedImage)\n",
    "\n",
    "        goal = findGoal(capturedImage)\n",
    "        \n",
    "        print(goal)\n",
    "\n",
    "        cv2.imshow('Thymio Camera', capturedImage)\n",
    "        \n",
    "\n",
    "        path=buildGraph(greenObjects,position,goal)\n",
    "        pointCount = 0\n",
    "        print(path)\n",
    "\n",
    "        if redoPath:\n",
    "            redoPath = False\n",
    "            continue\n",
    "        environmentSetup = True\n",
    "        \n",
    "        #cv2.waitKey(0)  # Wait until any key is pressed to close the image window\n",
    "\n",
    "\n",
    "\n",
    "    if environmentSetup:\n",
    "\n",
    "        if np.linalg.norm(position - path[pointCount]) < POSITION_THRESHOLD:\n",
    "            aligned = False\n",
    "            pointCount += 1\n",
    "            motors(0,0)\n",
    "\n",
    "            if pointCount == len(path):\n",
    "                motors(0,0)\n",
    "                print(\"Goal reached\")\n",
    "                break\n",
    "\n",
    "            continue\n",
    "        \n",
    "        if not aligned:\n",
    "            lSpeed,rSpeed,distanceError,angleError = robotController(path[pointCount-1],path[pointCount],position,angle,dt,alignMode=True)\n",
    "            motors(lSpeed,rSpeed)\n",
    "            if np.abs(angleError) < 0.1:\n",
    "                aligned = True\n",
    "                motors(0,0)\n",
    "            continue\n",
    "\n",
    "\n",
    "        lSpeed,rSpeed,distanceError,angleError = robotController(path[pointCount-1],path[pointCount],position,angle,dt,alignMode=False)\n",
    "        motors(lSpeed,rSpeed)\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motors(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "The primary objective of this Project was to develop and integrate various modules pertinent to mobile robotics. The synergy of these modules enabled the Thymio robot to navigate autonomously towards a designated target. This included the capability to circumvent unforeseen obstacles, manage the loss of camera input, and recover from dislocation incidents (commonly referred to as 'robot kidnapping').\n",
    "\n",
    "In essence, this project offered a comprehensive exploration of the multifaceted processes integral to mobile robotics. It served as an invaluable learning experience, enriching our understanding in both technical aspects of robotics and the dynamics of collaborative teamwork. This venture not only enhanced our technical expertise but also deepened our understanding of the intricate challenges and complexities that are characteristic of mobile robotics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sources**\n",
    "\n",
    "Course \"Basics of Mobile Robotics\" by Prof. Francesco Mondada (MICRO-452): https://moodle.epfl.ch/course/view.php?id=15293\n",
    "\n",
    "Course \"Multivariable Control\" by Prof. Giancarlo Ferrari Trecate (ME-422): https://moodle.epfl.ch/course/view.php?id=15777\n",
    "\n",
    "Elements of Robotics, by Mordechai Ben-Ari and Francesco Mondada Springer, 2018: https://link.springer.com/book/10.1007/978-3-319-62533-1\n",
    "\n",
    "Automatic Addison: https://automaticaddison.com\n",
    "\n",
    "mouhknowsbest (Youtube Channel): https://www.youtube.com/watch?v=aE7RQNhwnPQ\n",
    "\n",
    "Chat-GPT4: https://chat.openai.com/\n",
    "\n",
    "Copilot: https://copilot.github.com/\n",
    "\n",
    "Numpy: https://numpy.org/doc/\n",
    "\n",
    "OpenCV: https://docs.opencv.org/\n",
    "\n",
    "Pyvisgraph: https://github.com/TaipanRex/pyvisgraph/tree/master\n",
    "\n",
    "matplotlib: https://matplotlib.org/stable/contents.html\n",
    "\n",
    "TDM Client: https://github.com/epfl-mobots/tdm-python/tree/main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile_robotics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
